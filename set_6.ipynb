import os
import cv2
import numpy as np
import xml.etree.ElementTree as ET
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

# Path to the dataset
dataset_dir = "/content/drive/MyDrive/BCCD_Dataset-1.0/BCCD"  # Replace with the actual path to the BCCD dataset
classes = ['WBC', 'RBC', 'Platelets']
image_size = (64, 64)  # Resize images to 64x64

# Function to parse XML annotation files and get image labels
def parse_xml(xml_file):
    tree = ET.parse(xml_file)
    root = tree.getroot()
    labels = []
    
    # Extract all objects from the annotation file
    for obj in root.findall('object'):
        label = obj.find('name').text
        if label in classes:
            labels.append(label)
    
    return labels

# Step a: Load the dataset
images = []
labels = []

for filename in os.listdir(os.path.join(dataset_dir, 'JPEGImages')):
    if filename.endswith('.jpg'):
        image_path = os.path.join(dataset_dir, 'JPEGImages', filename)
        xml_path = os.path.join(dataset_dir, 'Annotations', filename.replace('.jpg', '.xml'))
        
        if os.path.exists(xml_path):
            image = cv2.imread(image_path)
            image = cv2.resize(image, image_size)
            images.append(image)
            
            # Get label from XML annotation
            img_labels = parse_xml(xml_path)
            if img_labels:
                # Assuming one label per image (you can modify this for multi-label classification)
                labels.append(classes.index(img_labels[0]))  # Take the first label found
            else:
                # If no label is found, skip the image
                continue

images = np.array(images)
labels = np.array(labels)

# Step b: Show the number of testing and training images
X_train, X_test, y_train, y_test = train_test_split(images, labels, test_size=0.2, random_state=42)
print(f"Training images: {len(X_train)}")
print(f"Testing images: {len(X_test)}")

# Step c: Plot some images
plt.figure(figsize=(10, 5))
for i in range(9):
    plt.subplot(3, 3, i+1)
    plt.imshow(X_train[i])
    plt.title(f"Class: {classes[y_train[i]]}")
    plt.axis('off')
plt.show()

# Step d: Image augmentation (contrast, flipping, rotation)
datagen = ImageDataGenerator(
    rotation_range=30,  # Random rotations
    width_shift_range=0.2,  # Horizontal shift
    height_shift_range=0.2,  # Vertical shift
    shear_range=0.2,  # Shearing
    zoom_range=0.2,  # Zooming
    horizontal_flip=True,  # Flipping
    vertical_flip=False,  # You can flip vertically if necessary
    brightness_range=[0.5, 1.5],  # Contrast adjustment
    fill_mode='nearest'  # How to fill missing pixels after transformations
)

# Step e: After augmentation, show the number of training and testing images
augmented_train_generator = datagen.flow(X_train, y_train, batch_size=32)
print(f"Number of training images after augmentation: {augmented_train_generator.n}")


# Step f: Normalize the training and testing data
X_train_norm = X_train / 255.0
X_test_norm = X_test / 255.0

# Step g: Build the CNN model
model = Sequential()
model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)))
model.add(MaxPooling2D((2, 2)))
model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(MaxPooling2D((2, 2)))
model.add(Conv2D(128, (3, 3), activation='relu'))
model.add(MaxPooling2D((2, 2)))
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(len(classes), activation='softmax'))

model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Step h: Train the model and show training and testing accuracy
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

history_before_augmentation = model.fit(
    X_train_norm, y_train, epochs=30, batch_size=32, validation_data=(X_test_norm, y_test), callbacks=[early_stopping]
)

# Plot training and validation accuracy before augmentation
plt.plot(history_before_augmentation.history['accuracy'], label='Training accuracy (before augmentation)')
plt.plot(history_before_augmentation.history['val_accuracy'], label='Validation accuracy (before augmentation)')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Training and Validation Accuracy Before Augmentation')
plt.legend()
plt.show()

# Step i: Train the model again with augmented data
history_after_augmentation = model.fit(
    augmented_train_generator, epochs=30, validation_data=(X_test_norm, y_test), callbacks=[early_stopping]
)

# Plot training and validation accuracy after augmentation
plt.plot(history_after_augmentation.history['accuracy'], label='Training accuracy (after augmentation)')
plt.plot(history_after_augmentation.history['val_accuracy'], label='Validation accuracy (after augmentation)')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Training and Validation Accuracy After Augmentation')
plt.legend()
plt.show()

# Step j: Show the training and testing accuracy after augmentation
print(f"Final training accuracy (after augmentation): {history_after_augmentation.history['accuracy'][-1]}")
print(f"Final testing accuracy (after augmentation): {history_after_augmentation.history['val_accuracy'][-1]}")

# Step k: Compare training and testing accuracy before and after augmentation
before_train_acc = history_before_augmentation.history['accuracy'][-1]
before_val_acc = history_before_augmentation.history['val_accuracy'][-1]
after_train_acc = history_after_augmentation.history['accuracy'][-1]
after_val_acc = history_after_augmentation.history['val_accuracy'][-1]

print(f"Before Augmentation - Training Accuracy: {before_train_acc}, Testing Accuracy: {before_val_acc}")
print(f"After Augmentation - Training Accuracy: {after_train_acc}, Testing Accuracy: {after_val_acc}")





















import os
import cv2
import xml.etree.ElementTree as ET
import matplotlib.pyplot as plt

# Path to the dataset
dataset_dir = '/content/drive/MyDrive/BCCD_Dataset-1.0/BCCD'  # Replace with the actual path to the BCCD dataset
image_size = (640, 480)  # Original image size, 640x480 for the BCCD dataset

# Function to parse XML annotation files and extract bounding box and label information
def parse_xml_for_annotations(xml_file):
    tree = ET.parse(xml_file)
    root = tree.getroot()
    annotations = []
    
    # Extract bounding boxes and labels for each object in the annotation
    for obj in root.findall('object'):
        label = obj.find('name').text
        bndbox = obj.find('bndbox')
        xmin = int(bndbox.find('xmin').text)
        ymin = int(bndbox.find('ymin').text)
        xmax = int(bndbox.find('xmax').text)
        ymax = int(bndbox.find('ymax').text)
        annotations.append((label, xmin, ymin, xmax, ymax))
    
    return annotations

# Load and preprocess the dataset, and visualize annotations
for filename in os.listdir(os.path.join(dataset_dir, 'JPEGImages')):
    if filename.endswith('.jpg'):
        image_path = os.path.join(dataset_dir, 'JPEGImages', filename)
        xml_path = os.path.join(dataset_dir, 'Annotations', filename.replace('.jpg', '.xml'))
        
        if os.path.exists(xml_path):
            # Load the image
            image = cv2.imread(image_path)
            
            # Get the annotations (bounding boxes and labels)
            annotations = parse_xml_for_annotations(xml_path)
            
            # Draw bounding boxes and labels on the image
            for label, xmin, ymin, xmax, ymax in annotations:
                # Draw the bounding box
                cv2.rectangle(image, (xmin, ymin), (xmax, ymax), (0, 255, 0), 2)
                # Add the label
                cv2.putText(image, label, (xmin, ymin - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)
            
            # Resize for visualization if needed (optional)
            image_resized = cv2.resize(image, (640, 480))  # Resize for display if necessary
            
            # Display the image with annotations
            plt.figure(figsize=(10, 10))
            plt.imshow(cv2.cvtColor(image_resized, cv2.COLOR_BGR2RGB))
            plt.title(f"Annotations for {filename}")
            plt.axis('off')
            plt.show()
        
        # You can limit the number of images shown if needed (for example, only 5 images)
        break  # Remove this line to display all images
(*  *)
